# 1.线程池结构
线程池模块中有工作线程和事件请求处理线程，工作线程默认有8个，事件请求处理线程只有1个  
为每个工作线程都维持一个工作任务队列，事件请求处理线程会将任务分配到每个线程的工作任务队列上  
# 2.线程池工作流程
每当epoll监听到套接字上有新的可读或可写事件发生时就将该事件传递到事件请求处理线程上，为了提高效率，事件请求处理线程会维持两个事件队列，分别是事件请求缓冲队列和事件请求处理队列，只要有新的事件请求，就投入事件请求缓冲队列，事件处理分配器处理完当前任务先对事件请求缓冲队列加锁，然后读取事件请求缓冲队列，随后解锁，处理事件请求处理队列，这样可以避免频繁的加锁和解锁。    

建立两个事件请求队列的目的是为了解决此问题：主线程和工作线程共享请求队列，对请求队列的操作需求加锁，耗费CPU时间  
通过建立两个事件队列，用类似缓冲的方法实现批量读取，缓和主线程和线程池对于请求队列的资源竞争。  


```c++
extern "C" {
  #include <stdio.h>
  #include <stdlib.h>
  #include "unpthread.h"
}

#include "locker.h"
#include <iostream>
#include <queue>

template <class T>
class Threadpool {
public:
	Threadpool(int default_thread_number=8 ); //默认8个线程

	virtual ~Threadpool();

	bool append(T*);

private:
	pthread_key_t r1_key; //线程自带数据
	pthread_once_t r1_once = PTHREAD_ONCE_INIT; //线程自带数据
	void thread_data_destructor(void *); //线程自带数据
	void thread_data_once(void); //线程自带数据

	static void *worker(void *); //工作线程，注意为静态成员函数
	static void *dealer(void *); //事件请求处理线程，注意为静态成员函数
	void worker_run(void *); //工作线程实体
	void dealer_run(); //事件请求处理线程实体
	int deal_request(T*); //事件请求处理函数

	int thread_number; //线程池线程个数
	pthread_t *threads; //记录线程号
	std::queue<T*> *single_work_queue; //为每个工作线程建立一个工作任务队列
	locker	*single_work_mutex; //为每个工作任务队列建立一个互斥锁
	sem *single_queue_stat; //为每个工作线程建立一个信号量
	volatile bool flag_stop; //进程池终止标志位,使用volatile

	std::queue<T*> buff_work_queue; //总的事件请求缓冲队列
	int buff_request_number; //事件请求的个数
	pthread_cond_t deal_cond; //事件请求条件变量
	std::queue<T*> deal_work_queue; //总的事件请求处理队列
	locker buff_queue_lock;
	locker deal_queue_lock;
	pthread_t deal_thread; //事件请求处理线程号
	sem deal_stat; //事件请求处理线程信号量

	typedef struct
	{
		 Threadpool *pool; //因为worker()、dealer()函数为静态成员函数，所以要传类指针进去
	    int thread_id;
	}send_data;  //用于线程池创建时将线程信息传进去
};

template <class T>
Threadpool<T>::Threadpool(int default_thread_number):thread_number(default_thread_number),flag_stop(false),
threads(NULL),single_work_queue(NULL),single_work_mutex(NULL),single_queue_stat(NULL),buff_request_number(0)
{
	if(thread_number<0)
	{
		err_quit("thread_number error");
	}

	threads=new pthread_t[thread_number];
	if(!threads)
	{
		err_quit("threads error");
	}

	single_work_queue=new std::queue<T*>[thread_number];
	if(!single_work_queue)
	{
		err_quit("single_work_queue error");
	}

	single_work_mutex=new locker[thread_number];
	if(!single_work_mutex)
	{
		err_quit("single_work_mutex error");
	}

	single_queue_stat=new sem[thread_number];
	if(!single_queue_stat)
	{
		err_quit("single_queue_stat error");
	}

	for(int i=0;i<thread_number;++i) //建立各工作线程
	{
		printf("create %d thread\n",i);
		send_data *temp; //send_data用于传递数据
		temp->pool=this;
		temp->thread_id=i;
		if( pthread_create(threads+i,NULL,worker,(void*)temp) >0 ) //建立各工作线程，temp中存储要传递的数据
		{
			delete []threads;
			delete []single_work_queue;
			delete []single_work_mutex;
			delete []single_queue_stat;
			err_quit("phread_create error");
		}
		if( pthread_detach(threads[i]) >0 ) //分离各工作线程
		{
			delete []threads;
			delete []single_work_queue;
			delete []single_work_mutex;
			delete []single_queue_stat;
			err_quit("phread_detach error");
		}
	}

	if( pthread_create(&deal_thread,NULL,dealer,this) >0 ) //建立事件请求处理线程
	{
		delete []threads;
		delete []single_work_queue;
		delete []single_work_mutex;
		delete []single_queue_stat;
		err_quit("deal pthread_create error");
	}

	if( pthread_detach(deal_thread) >0 )  //分离事件请求处理线程
	{
		delete []threads;
		delete []single_work_queue;
		delete []single_work_mutex;
		delete []single_queue_stat;
		err_quit("deal pthread_create error");
	}

	if( pthread_cond_init(&deal_cond,NULL) >0) //初始化条件变量
	{
		delete []threads;
		delete []single_work_queue;
		delete []single_work_mutex;
		delete []single_queue_stat;
		err_quit("pthread_cond_init error");
	}
}

template <class T>
Threadpool<T>::~Threadpool()
{
	delete []threads;
	delete []single_work_queue;
	delete []single_work_mutex;
	delete []single_queue_stat;
	flag_stop=true;
}

template< class T >
bool Threadpool< T >::append( T* request ) //向事件请求缓冲队列中添加一个事件请求
{
	//注意要对事件请求缓冲队列上锁
	if(request)
	{
		 buff_queue_lock.lock();
	    buff_work_queue.push_back( request );
	    ++buff_request_number;
	    Pthread_cond_signal(&deal_cond); //发出信号
	    buff_queue_lock.unlock();
	    return true;
	}
	else
	{
		return false;
	}
}

template< class T >
void* Threadpool< T >::worker( void* arg )
{
	 send_data *temp=(send_data *)arg;  //对传入的数据分解
    Threadpool* pool =temp->pool;
    pool->worker_run(arg);
    return pool;
}

template< class T >
void* Threadpool< T >::dealer( void* arg )
{
    Threadpool* pool = ( Threadpool* )arg;
    pool->dealer_run();
    return pool;
}

template< class T >
void Threadpool< T >::dealer_run()
{
	while(!flag_stop)
	{
		buff_queue_lock.lock();
		while(buff_request_number == 0)
			Pthread_cond_wait(&deal_cond,&buff_queue_lock.m_mutex);

		while(!buff_work_queue.empty()) //将buff_work_queue中的请求转到deal_work_queue
		{
			deal_work_queue.push_back(buff_work_queue.front());
			buff_work_queue.pop();
			--buff_request_number; //记得减去,否则cond的触发会有问题，注意是批量读取
		}
		buff_queue_lock.unlock();

		deal_queue_lock.lock();
		//处理deal_work_queue中的事件请求，随后将其分给各工作线程的工作队列，并通知各工作线程
		for(static int i=0;!deal_work_queue.empty();++i) // static int i  下次进入时仍然记得之前分配的位置
		{
			T* request = deal_work_queue.front();
			if(!request)
				continue;
			if( deal_request(request) )//处理请求
			{
				//此处只是简单的循环分发任务
				i%=thread_number;
				single_work_mutex[i].lock();
				single_work_queue[i].push_back(request);
				single_work_mutex[i].unlock();
				single_queue_stat[i].post();  //注意是先将任务放到工作队列上以后再通知相应的工作线程
			}
			else
			{
				--i;
			}
		}
		deal_queue_lock.unlock();
	}
}

template< class T >
int Threadpool< T >::deal_request(T*)
{
	return 1;
}

template< class T >
void Threadpool< T >::thread_data_destructor(void *ptr)  //释放线程自带数据
{
	free(ptr);
}

template< class T >
void Threadpool< T >::thread_data_once(void)
{
	Pthread_key_create(&r1_key,thread_data_destructor);
}

template< class T >
void Threadpool< T >::worker_run(void *arg)
{
	//先建立线程自带数据
	Pthread_once(&r1_once,thread_data_once);
	int *ptr;
	if( (ptr = (int *)pthread_getspecific(r1_key)) == NULL)
	{
		ptr=(int *)Malloc(sizeof(int));
		Pthread_setspecific(r1_key,ptr);
	}
	send_data *temp=(send_data *)arg;  //对传入的数据分解
	*ptr=temp->thread_id; //在线程自带数据中记录线程id
	delete temp; //回收动态分配的内存

	//线程处理自己工作队列上的任务
	 while ( ! flag_stop )
	 {
		 single_queue_stat[*(int*)ptr].wait();
		 single_work_mutex[*(int*)ptr].lock();
		 if(single_work_queue[*(int*)ptr].isempty())
		 {
			 single_work_mutex[*(int*)ptr].unlock();
			 printf("single_work_queue miss request"); //正常不可能丢失任务
			 continue;
		 }
		 while(!single_work_queue[*(int*)ptr].isempty()) //循环处理工作队列上的任务
		 {
			 T *request;
			 request=single_work_queue[*(int*)ptr].front();
			 single_work_queue[*(int*)ptr].pop();
			 single_work_mutex[*(int*)ptr].unlock(); //一旦找到当前的任务就解锁，方便事件请求处理线程分配新的工作

			 if(!request) //判断是否是无效任务
			 {
				 if(!single_work_queue[*(int*)ptr].isempty())//如果当前队列为空则直接跳出，不上锁，上锁会导致无法解锁，如果不为空，则上锁
				 {
					 single_work_mutex[*(int*)ptr].lock(); //注意先上锁
					 continue;
				 }
				 else
					 break;
			 }

			 request->process();  //预计是一个很耗费时间的工作

			 if(!single_work_queue[*(int*)ptr].isempty()) //工作处理完后如果当前的工作队列还有任务，则上锁后再去处理，否则直接跳出
			 {
				 single_work_mutex[*(int*)ptr].lock(); //注意先上锁
				 continue;
			 }
			 else
				 break;
			 }
	 }
}
```
